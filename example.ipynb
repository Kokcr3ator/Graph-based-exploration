{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from four_room_grid import Four_room_grid,Policy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "grid = Four_room_grid()\n",
    "grid.plot_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(n_iter = 10):\n",
    "    policy = Policy()\n",
    "    for iter in range(n_iter):\n",
    "        if (iter+1) % 5 == 0:\n",
    "            print(f'iteration {iter+1}')\n",
    "            grid.plot_grid(policy.matrix)\n",
    "        Q_pi = policy.calculate_Q_pi()\n",
    "        new_policy = np.zeros((grid.n_navigable_states,grid.n_actions))\n",
    "        for idx,state in enumerate(np.argmax(Q_pi, axis = 1)):\n",
    "            new_policy[idx,state] = 1\n",
    "        policy = Policy(new_policy)\n",
    "    return policy\n",
    "\n",
    "optimal_policy = policy_iteration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g,A = grid.create_state_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.plot_state_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = grid.laplacian_state()\n",
    "norm_L = grid.normalized_laplacian_state()\n",
    "U_combinatorial, _, _ = np.linalg.svd(L)\n",
    "U_norm, _, _ = np.linalg.svd(norm_L)\n",
    "k_vec = [k+1 for k in range(50)]\n",
    "err_vec_combinatorial = []\n",
    "err_vec_normalized = []\n",
    "uniform_policy = Policy()\n",
    "V_uniform = uniform_policy.calculate_V_pi()\n",
    "for k in k_vec:\n",
    "    phi = U_combinatorial[:,-k:]\n",
    "    weights = np.linalg.solve(phi.T@phi, phi.T@V_uniform)\n",
    "    V_predicted = phi @ weights\n",
    "    err_vec_combinatorial.append(np.mean((V_predicted - V_uniform)**2))\n",
    "\n",
    "    phi = U_norm[:,-k:]\n",
    "    weights = np.linalg.solve(phi.T@phi, phi.T@V_uniform)\n",
    "    V_predicted = phi @ weights\n",
    "    err_vec_normalized.append(np.mean((V_predicted - V_uniform)**2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_vec, err_vec_combinatorial, marker='o', label = 'Combinatorial Laplacian')\n",
    "plt.plot(k_vec, err_vec_normalized, marker='v', label = 'Normalized Laplacian')\n",
    "plt.yscale('log')\n",
    "plt.title('V of uniform policy approximation using PVF')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('MSE')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g,A = grid.create_state_action_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.plot_state_action_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.set_policy(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G,W = grid.create_state_action_graph(weighted = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.plot_state_action_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_Q_pi_using_PVF(k, target_pi, normalized = False, weight_policy = None, return_Q_target = False):\n",
    "    grid = Four_room_grid()\n",
    "    Q_target = target_pi.calculate_Q_pi(as_vector= True)\n",
    "    if weight_policy is None:\n",
    "        if normalized:\n",
    "            L = grid.normalized_laplacian_state_action()\n",
    "        else:\n",
    "            L = grid.laplacian_state_action()\n",
    "    else:\n",
    "        grid.set_policy(weight_policy)\n",
    "        if normalized:\n",
    "            L = grid.normalized_laplacian_state_action(weighted = True)\n",
    "        else:\n",
    "            L = grid.laplacian_state_action(weighted= True)        \n",
    "\n",
    "    U, _, _ = np.linalg.svd(L)\n",
    "    phi = U[:,-k:]\n",
    "    weights = np.linalg.solve(phi.T@phi, phi.T@Q_target)\n",
    "    Q_predicted = phi @ weights\n",
    "    \n",
    "    if return_Q_target:\n",
    "        return Q_predicted, Q_target\n",
    "    else:\n",
    "        return Q_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_approximation_error(target_pi, weight_policy):\n",
    "    err_vec_weighted_normalized = []\n",
    "    err_vec_unweighted_normalized = []\n",
    "    err_vec_weighted = []\n",
    "    err_vec_unweighted = []\n",
    "    k_vec = [5,10,50,75,100,125,150,175,200]\n",
    "    for k in k_vec:\n",
    "        Q_pred, Q_target = approximate_Q_pi_using_PVF(k = k, target_pi= target_pi, weight_policy= weight_policy, normalized= True, return_Q_target= True)\n",
    "        err_vec_weighted_normalized.append(np.mean((Q_pred - Q_target)**2))\n",
    "        Q_pred, Q_target = approximate_Q_pi_using_PVF(k = k, target_pi= target_pi, normalized= True, return_Q_target= True)\n",
    "        err_vec_unweighted_normalized.append(np.mean((Q_pred - Q_target)**2))\n",
    "        Q_pred, Q_target = approximate_Q_pi_using_PVF(k = k, target_pi= target_pi, weight_policy= weight_policy, normalized= False, return_Q_target= True)\n",
    "        err_vec_weighted.append(np.mean((Q_pred - Q_target)**2)) \n",
    "        Q_pred, Q_target = approximate_Q_pi_using_PVF(k = k, target_pi= target_pi, normalized= False, return_Q_target= True)\n",
    "        err_vec_unweighted.append(np.mean((Q_pred - Q_target)**2))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_vec, err_vec_weighted_normalized, marker='o', label = 'Weighted using normalized Laplacian')\n",
    "    plt.plot(k_vec, err_vec_unweighted_normalized, marker='v', label = 'Unweighted using normalized Laplacian')\n",
    "    plt.plot(k_vec, err_vec_weighted, marker='s', label = 'Weighted using combinatorial Laplacian')\n",
    "    plt.plot(k_vec, err_vec_unweighted, marker='*', label = 'Unweighted using combinatorial Laplacian')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Q_pi  approximation')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_approximation_error(target_pi = optimal_policy, weight_policy = optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_approximation_error(target_pi = uniform_policy, weight_policy = uniform_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random deterministic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_vector(array):\n",
    "    idx_max = np.argmax(array)\n",
    "    new_vec = np.zeros(len(array))\n",
    "    new_vec[idx_max] = 1\n",
    "    return new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_deterministic_policy_matrix = np.random.rand(grid.n_navigable_states, grid.n_actions)\n",
    "random_deterministic_policy_matrix = np.array([argmax_vector(row) for row in random_deterministic_policy_matrix])\n",
    "\n",
    "random_deterministic_policy = Policy(random_deterministic_policy_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_approximation_error(target_pi = random_deterministic_policy, weight_policy = random_deterministic_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random stochastic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_stochastic_policy_matrix = np.random.rand(grid.n_navigable_states, grid.n_actions)\n",
    "random_stochastic_policy_matrix = np.array([row/sum(row) for row in random_stochastic_policy_matrix])\n",
    "random_stochastic_policy = Policy(random_stochastic_policy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_approximation_error(target_pi = random_stochastic_policy, weight_policy = random_stochastic_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way I am also predicting the Q function for the absorbing state, what if I ignore it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_Q_pi_using_PVF(k, target_pi, normalized = False, weight_policy = None, return_Q_target = False):\n",
    "    grid = Four_room_grid()\n",
    "    Q_target = target_pi.calculate_Q_pi(as_vector= True)\n",
    "    Q_target = np.delete(Q_target, np.s_[-32:-28])\n",
    "    \n",
    "    if weight_policy is None:\n",
    "        if normalized:\n",
    "            L = grid.normalized_laplacian_state_action()\n",
    "        else:\n",
    "            L = grid.laplacian_state_action()\n",
    "    else:\n",
    "        grid.set_policy(weight_policy)\n",
    "        if normalized:\n",
    "            L = grid.normalized_laplacian_state_action(weighted = True)\n",
    "        else:\n",
    "            L = grid.laplacian_state_action(weighted= True)        \n",
    "\n",
    "    U, _, _ = np.linalg.svd(L)\n",
    "    phi = U[:,-k:]\n",
    "    phi = np.delete(phi, np.s_[-32:-28], 0)\n",
    "    weights = np.linalg.solve(phi.T@phi, phi.T@Q_target)\n",
    "    Q_predicted = phi @ weights\n",
    "    \n",
    "    if return_Q_target:\n",
    "        return Q_predicted, Q_target\n",
    "    else:\n",
    "        return Q_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_approximation_error(target_pi = optimal_policy, weight_policy = optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_approximation_error(target_pi = uniform_policy, weight_policy = uniform_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_approximation_error(target_pi = random_deterministic_policy, weight_policy = random_deterministic_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_approximation_error(target_pi = random_stochastic_policy, weight_policy = random_stochastic_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
